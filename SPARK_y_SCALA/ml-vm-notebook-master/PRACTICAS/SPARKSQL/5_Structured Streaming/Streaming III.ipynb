{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18b51570",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://iblaesmadc00744.iecisa.corp:4044\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1681216608679)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n",
       "import spark.implicits._\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/11 14:37:03 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\r\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4d86723",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.types._\r\n",
       "import org.apache.spark.sql.functions._\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.types._\n",
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdab50db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "schema: org.apache.spark.sql.types.StructType = StructType(StructField(nombre,StringType,true),StructField(edad,LongType,true),StructField(sexo,StringType,true))\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val schema = new StructType().add(\"nombre\", \"string\").add(\"edad\", \"long\").add(\"sexo\", \"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ed48332",
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.UnsatisfiedLinkError",
     "evalue": " org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r",
     "output_type": "error",
     "traceback": [
      "java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r",
      "  at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r",
      "  at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r",
      "  at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r",
      "  at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r",
      "  at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r",
      "  at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1972)\r",
      "  at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:2014)\r",
      "  at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:761)\r",
      "  at org.apache.spark.util.HadoopFSUtils$.listLeafFiles(HadoopFSUtils.scala:225)\r",
      "  at org.apache.spark.util.HadoopFSUtils$.$anonfun$parallelListLeafFilesInternal$1(HadoopFSUtils.scala:95)\r",
      "  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\r",
      "  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\r",
      "  at scala.collection.AbstractTraversable.map(Traversable.scala:108)\r",
      "  at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFilesInternal(HadoopFSUtils.scala:85)\r",
      "  at org.apache.spark.util.HadoopFSUtils$.parallelListLeafFiles(HadoopFSUtils.scala:69)\r",
      "  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex$.bulkListLeafFiles(InMemoryFileIndex.scala:158)\r",
      "  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.listLeafFiles(InMemoryFileIndex.scala:131)\r",
      "  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.refresh0(InMemoryFileIndex.scala:94)\r",
      "  at org.apache.spark.sql.execution.datasources.InMemoryFileIndex.<init>(InMemoryFileIndex.scala:66)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource.createInMemoryFileIndex(DataSource.scala:567)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource.$anonfun$sourceSchema$2(DataSource.scala:268)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$lzycompute$1(DataSource.scala:164)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource.tempFileIndex$1(DataSource.scala:164)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource.getOrInferFileFormatSchema(DataSource.scala:169)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:262)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:118)\r",
      "  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:118)\r",
      "  at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:34)\r",
      "  at org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:196)\r",
      "  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:210)\r",
      "  at org.apache.spark.sql.streaming.DataStreamReader.json(DataStreamReader.scala:236)\r",
      "  ... 44 elided\r",
      ""
     ]
    }
   ],
   "source": [
    "val personasDF = spark.readStream\n",
    "    .schema(schema)\n",
    "    .json(\"C:\\\\Users\\\\javier.puente.ext\\\\Documents\\\\SPARK\\\\ml-vm-notebook-master\\\\PRACTICAS\\\\SPARKSQL\\\\personas\")\n",
    "\n",
    "personasDF.createOrReplaceTempView(\"personas\")\n",
    "val mediaEdad = spark.sql(\"select sexo, avg(edad) from personas group by sexo\")\n",
    "val query = mediaEdad.writeStream\n",
    ".format(\"parquet\")\n",
    ".outputMode(\"complete\")\n",
    ".option(\"path\", \"C:\\\\Users\\\\javier.puente.ext\\\\Documents\")\n",
    ".start()\n",
    "\n",
    "query.awaitTermination(90000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2037bd15",
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "33: error: not found: value mediaEdad\r",
     "output_type": "error",
     "traceback": [
      "<console>:33: error: not found: value mediaEdad\r",
      "       val query2 = mediaEdad.writeStream\r",
      "                    ^\r",
      ""
     ]
    }
   ],
   "source": [
    "val query2 = mediaEdad.writeStream\n",
    ".format(\"parquet\")\n",
    ".outputMode(\"append\")\n",
    ".option(\"path\", \"C:\\\\Users\\\\javier.puente.ext\\\\Documents\")\n",
    ".option(\"checkpointLocation\", \"C:\\\\Users\\\\javier.puente.ext\\\\Documents\")\n",
    ".start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
