{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5a6facd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://iblaesmadc00744.iecisa.corp:4042\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1681211746219)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@2034782b\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2546bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/11 13:33:41 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.Row\r\n",
       "import spark.implicits._\r\n",
       "lineas: org.apache.spark.sql.DataFrame = [value: string]\r\n",
       "res1: Boolean = true\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.Row\n",
    "import spark.implicits._\n",
    "val lineas = spark.readStream\n",
    ".format(\"socket\")\n",
    ".option(\"host\", \"localhost\")\n",
    ".option(\"port\", 9999)\n",
    ".load()\n",
    "\n",
    "lineas.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6a2e45d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "palabras: org.apache.spark.sql.Dataset[String] = [value: string]\r\n",
       "numpalabras: org.apache.spark.sql.DataFrame = [value: string, count: bigint]\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val palabras = lineas.as[String].flatMap(_.split(\" \"))\n",
    "\n",
    "val numpalabras = palabras.groupBy(\"value\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74118279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/11 13:55:59 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: C:\\Users\\javier.puente.ext\\AppData\\Local\\Temp\\temporary-b3e4adae-4af1-46f1-93fe-10607e02c5a7. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/04/11 13:55:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "23/04/11 13:56:00 ERROR MicroBatchExecution: Query [id = 3875fae8-d619-41a6-919a-8e90cf0314f2, runId = 88a10d95-8019-4517-96de-0f557f244b41] terminated with error\n",
      "java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\n",
      "\tat org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\n",
      "\tat org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.listStatus(DelegateToFileSystem.java:177)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.listStatus(ChecksumFs.java:548)\n",
      "\tat org.apache.hadoop.fs.FileContext$Util$1.next(FileContext.java:1915)\n",
      "\tat org.apache.hadoop.fs.FileContext$Util$1.next(FileContext.java:1911)\n",
      "\tat org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\n",
      "\tat org.apache.hadoop.fs.FileContext$Util.listStatus(FileContext.java:1917)\n",
      "\tat org.apache.hadoop.fs.FileContext$Util.listStatus(FileContext.java:1876)\n",
      "\tat org.apache.hadoop.fs.FileContext$Util.listStatus(FileContext.java:1835)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.list(CheckpointFileManager.scala:316)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.getLatestBatchId(HDFSMetadataLog.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.HDFSMetadataLog.getLatest(HDFSMetadataLog.scala:220)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:223)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\n"
     ]
    },
    {
     "ename": "org.apache.spark.sql.streaming.StreamingQueryException",
     "evalue": " Query [id = 3875fae8-d619-41a6-919a-8e90cf0314f2, runId = 88a10d95-8019-4517-96de-0f557f244b41] terminated with exception: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.sql.streaming.StreamingQueryException: Query [id = 3875fae8-d619-41a6-919a-8e90cf0314f2, runId = 88a10d95-8019-4517-96de-0f557f244b41] terminated with exception: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:330)\r",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:208)\r",
      "Caused by: java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\r",
      "  at org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Native Method)\r",
      "  at org.apache.hadoop.io.nativeio.NativeIO$Windows.access(NativeIO.java:793)\r",
      "  at org.apache.hadoop.fs.FileUtil.canRead(FileUtil.java:1218)\r",
      "  at org.apache.hadoop.fs.FileUtil.list(FileUtil.java:1423)\r",
      "  at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:601)\r",
      "  at org.apache.hadoop.fs.DelegateToFileSystem.listStatus(DelegateToFileSystem.java:177)\r",
      "  at org.apache.hadoop.fs.ChecksumFs.listStatus(ChecksumFs.java:548)\r",
      "  at org.apache.hadoop.fs.FileContext$Util$1.next(FileContext.java:1915)\r",
      "  at org.apache.hadoop.fs.FileContext$Util$1.next(FileContext.java:1911)\r",
      "  at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90)\r",
      "  at org.apache.hadoop.fs.FileContext$Util.listStatus(FileContext.java:1917)\r",
      "  at org.apache.hadoop.fs.FileContext$Util.listStatus(FileContext.java:1876)\r",
      "  at org.apache.hadoop.fs.FileContext$Util.listStatus(FileContext.java:1835)\r",
      "  at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.list(CheckpointFileManager.scala:316)\r",
      "  at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.getLatestBatchId(HDFSMetadataLog.scala:213)\r",
      "  at org.apache.spark.sql.execution.streaming.HDFSMetadataLog.getLatest(HDFSMetadataLog.scala:220)\r",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:223)\r",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:375)\r",
      "  at org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:373)\r",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:68)\r",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:219)\r",
      "  at org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\r",
      "  at org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:213)\r",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:307)\r",
      "  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r",
      "  at org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:285)\r",
      "  ... 1 more\r",
      ""
     ]
    }
   ],
   "source": [
    "val query = numpalabras.writeStream\n",
    ".format(\"console\")\n",
    ".outputMode(\"update\")\n",
    ".start()\n",
    "\n",
    "query.awaitTermination(60000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
