{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67353bb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://iblaesmadc00744.iecisa.corp:4042\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1681200835583)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/11 10:14:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\r\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\r\n",
       "spark: org.apache.spark.sql.SparkSession = org.apache.spark.sql.SparkSession@22bedbd7\r\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/11 10:14:13 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\r\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "val spark = SparkSession.builder.appName(\"Mi App 2\").master(\"localhost\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d72f348",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res0: org.apache.spark.SparkContext = org.apache.spark.SparkContext@134d8fbb\r\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e976a28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sc: org.apache.spark.SparkContext = org.apache.spark.SparkContext@134d8fbb\r\n",
       "texto: org.apache.spark.rdd.RDD[String] = C:/Users/javier.puente.ext/Documents/SPARK/ml-vm-notebook-master/PRACTICAS/SPARKSQL/Stranger_Things_Personajes.txt MapPartitionsRDD[1] at textFile at <console>:27\r\n",
       "res1: Array[String] = Array(Eleven, 12, M, Hopper, 45, H, Will, 11, H, Max, 11, M, Mike, 11, H, Lucas, 11, H, Dustin, 11, H, Eleven, 12, M, Nancy, 23, M, Jonathan, 24, H)\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val sc = spark.sparkContext\n",
    "val texto = sc.textFile(\"C:/Users/javier.puente.ext/Documents/SPARK/ml-vm-notebook-master/PRACTICAS/SPARKSQL/Stranger_Things_Personajes.txt\")\n",
    "texto.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e14be28f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class Personaje\r\n",
       "import spark.implicits._\r\n",
       "partes: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[2] at map at <console>:28\r\n",
       "res2: Array[Array[String]] = Array(Array(Eleven, \" 12\", \" M\"), Array(Hopper, \" 45\", \" H\"), Array(Will, \" 11\", \" H\"), Array(Max, \" 11\", \" M\"), Array(Mike, \" 11\", \" H\"), Array(Lucas, \" 11\", \" H\"), Array(Dustin, \" 11\", \" H\"), Array(Eleven, \" 12\", \" M\"), Array(Nancy, \" 23\", \" M\"), Array(Jonathan, \" 24\", \" H\"))\r\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case class Personaje(nombre: String, edad: Int, sexo: String)\n",
    "import spark.implicits._\n",
    "val partes = texto.map(_.split(\",\"))\n",
    "partes.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "020f43c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "personajes: org.apache.spark.sql.Dataset[Personaje] = [nombre: string, edad: int ... 1 more field]\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val personajes = partes.map(atr=> Personaje(atr(0), atr(1).toInt, atr(2))).toDS()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33678b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/04/11 10:23:47 ERROR Executor: Exception in task 0.0 in stage 3.0 (TID 5)\n",
      "java.lang.NoClassDefFoundError: Could not initialize class $line9.$read$\n",
      "\tat $line18.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.$anonfun$personajes$1(<console>:31)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "23/04/11 10:23:47 WARN TaskSetManager: Lost task 0.0 in stage 3.0 (TID 5) (iblaesmadc00744.iecisa.corp executor driver): java.lang.NoClassDefFoundError: Could not initialize class $line9.$read$\n",
      "\tat $line18.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.$anonfun$personajes$1(<console>:31)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "23/04/11 10:23:47 ERROR TaskSetManager: Task 0 in stage 3.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "org.apache.spark.SparkException",
     "evalue": " Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 5) (iblaesmadc00744.iecisa.corp executor driver): java.lang.NoClassDefFoundError: Could not initialize class \r",
     "output_type": "error",
     "traceback": [
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 3.0 failed 1 times, most recent failure: Lost task 0.0 in stage 3.0 (TID 5) (iblaesmadc00744.iecisa.corp executor driver): java.lang.NoClassDefFoundError: Could not initialize class \r",
      "\tat $anonfun$personajes$1(<console>:31)\r",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r",
      "\tat scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r",
      "\tat java.lang.Thread.run(Thread.java:748)\r",
      "\r",
      "Driver stacktrace:\r",
      "  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\r",
      "  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r",
      "  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r",
      "  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\r",
      "  at scala.Option.foreach(Option.scala:407)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\r",
      "  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\r",
      "  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r",
      "  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2238)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2259)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2278)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:506)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:459)\r",
      "  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)\r",
      "  at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3868)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2863)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:3858)\r",
      "  at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:510)\r",
      "  at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3856)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)\r",
      "  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)\r",
      "  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\r",
      "  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3856)\r",
      "  at org.apache.spark.sql.Dataset.head(Dataset.scala:2863)\r",
      "  at org.apache.spark.sql.Dataset.take(Dataset.scala:3084)\r",
      "  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)\r",
      "  at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:808)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:767)\r",
      "  at org.apache.spark.sql.Dataset.show(Dataset.scala:776)\r",
      "  ... 40 elided\r",
      "Caused by: java.lang.NoClassDefFoundError: Could not initialize class \r",
      "  at $anonfun$personajes$1(<console>:31)\r",
      "  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r",
      "  at scala.collection.Iterator$$anon$10.next(Iterator.scala:461)\r",
      "  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r",
      "  at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r",
      "  at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r",
      "  at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:364)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r",
      "  at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)\r",
      "  at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\r",
      "  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r",
      "  at org.apache.spark.scheduler.Task.run(Task.scala:136)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r",
      "  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\r",
      "  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r",
      "  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r",
      "  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r",
      "  ... 1 more\r",
      ""
     ]
    }
   ],
   "source": [
    "personajes.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
