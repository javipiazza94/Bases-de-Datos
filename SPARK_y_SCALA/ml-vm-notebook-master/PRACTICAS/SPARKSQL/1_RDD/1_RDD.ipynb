{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182ac9b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://IBLAESSEVC05187.IBL.INETUM:4043\n",
       "SparkContext available as 'sc' (version = 3.3.2, master = local[*], app id = local-1680265102438)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "<console>",
     "evalue": "2: error: identifier expected but '.' found.\r",
     "output_type": "error",
     "traceback": [
      "<console>:2: error: identifier expected but '.' found.\r",
      "       import.org.apache.spark.SparkConf\r",
      "             ^\r",
      "<console>:2: error: . expected\r",
      "       import.org.apache.spark.SparkConf\r",
      "                                        ^\r",
      "<console>:3: error: identifier expected but '.' found.\r",
      "       import.org.apache.spark.SparkContext\r",
      "             ^\r",
      "<console>:3: error: . expected\r",
      "       import.org.apache.spark.SparkContext\r",
      "                                           ^\r",
      "<console>:4: error: identifier expected but '.' found.\r",
      "       import.org.apache.spark.rdd.RDD\r",
      "             ^\r",
      ""
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/31 14:18:33 WARN ProcfsMetricsGetter: Exception when trying to compute pagesize, as a result reporting of ProcessTree metrics is stopped\r\n"
     ]
    }
   ],
   "source": [
    "import.org.apache.spark.SparkConf\n",
    "import.org.apache.spark.SparkContext\n",
    "import.org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b074f1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res1: Array[(String, String)] = Array((spark.driver.port,59991), (spark.app.id,local-1680265102438), (spark.driver.extraJavaOptions,-XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED ...\r\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf.getAll;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68b5d1c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cadena_array: Array[String] = Array(augusto, cesar, marco antonio, trajano)\r\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cadena_array = Array(\"augusto\", \"cesar\", \"marco antonio\", \"trajano\");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b894f567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cadenas_RDD: org.apache.spark.rdd.RDD[String] = ParallelCollectionRDD[0] at parallelize at <console>:25\r\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val cadenas_RDD = sc.parallelize(cadena_array, 2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2457b18a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res2: Array[String] = Array(augusto, cesar, marco antonio, trajano)\r\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cadenas_RDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e669b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numeros_RDD: org.apache.spark.rdd.RDD[Int] = ParallelCollectionRDD[1] at parallelize at <console>:24\r\n",
       "res3: Array[Int] = Array(1, 2, 3, 4, 5, 6, 7)\r\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val numeros_RDD = sc.parallelize(Array(1,2,3,4,5,6,7), 2);\n",
    "numeros_RDD.collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95bc1ed3",
   "metadata": {},
   "outputs": [
    {
     "ename": "org.apache.hadoop.mapred.InvalidInputException",
     "evalue": " Input path does not exist: file:/C:/Users/javier.puente.ext/Downloads/ml-vm-notebook-master/PRACTICAS/SPARKSQL/1_RDD/~/Documents/SPARK/RDDs\r",
     "output_type": "error",
     "traceback": [
      "org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/Users/javier.puente.ext/Downloads/ml-vm-notebook-master/PRACTICAS/SPARKSQL/1_RDD/~/Documents/SPARK/RDDs\r",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\r",
      "  at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r",
      "  at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r",
      "  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r",
      "  at scala.Option.getOrElse(Option.scala:189)\r",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r",
      "  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\r",
      "  at scala.Option.getOrElse(Option.scala:189)\r",
      "  at org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\r",
      "  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\r",
      "  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r",
      "  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r",
      "  at org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\r",
      "  at org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\r",
      "  ... 37 elided\r",
      "Caused by: java.io.IOException: Input path does not exist: file:/C:/Users/javier.puente.ext/Downloads/ml-vm-notebook-master/PRACTICAS/SPARKSQL/1_RDD/~/Documents/SPARK/RDDs\r",
      "  at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\r",
      "  ... 53 more\r",
      ""
     ]
    }
   ],
   "source": [
    "val archivo = sc.textFile(\"~/Documents/SPARK/RDDs\");\n",
    "archivo.collect()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4ae17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822fbc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
